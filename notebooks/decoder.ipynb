{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiR9DpMs4WB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SoU1b1C0b5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_of_features, dim_of_features, hidden_size, vocab_size, embedding_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.num_of_features = num_of_features\n",
        "        self.dim_of_features = dim_of_features\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.word_embed = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "        \n",
        "        # LSTM related variables\n",
        "        self.lstm_cell = nn.LSTMCell(embedding_size+dim_of_features, hidden_size)\n",
        "        self.deep_output = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        \n",
        "        \n",
        "    # Initializes the hidden and memory states of the LSTM    \n",
        "    def initialize_states(self, features):\n",
        "      mean_features = features.mean(dim=1)\n",
        "     \n",
        "      if self.dim_of_features!= self.hidden_size:\n",
        "        hidden_mlp_layer = nn.Linear(self.dim_of_features, self.hidden_size)\n",
        "        memory_mlp_layer = nn.Linear(self.dim_of_features, self.hidden_size)\n",
        "        # Initial states of the RNN are MLP outputs from a mean feature vector\n",
        "        h0 = hidden_mlp_layer(mean_features)\n",
        "        c0 = memory_mlp_layer(mean_features)\n",
        "\n",
        "      else:\n",
        "        h0 = mean_features\n",
        "        c0 = mean_features\n",
        "      return h0,c0\n",
        "\n",
        "    # Finds the feature to be focused for the current time step \n",
        "    def build_attention_model(self, features, hidden):\n",
        "      # To be added by Xenia\n",
        "      print(\"\")\n",
        "      \n",
        "      \n",
        "    # Runs the forward pass of the LSTM \n",
        "    def forward(self, img_features, captions, dropout =\n",
        "               False):\n",
        "      # Initializing the required variables\n",
        "      batch_size = img_features.size(0)\n",
        "      h0, c0 = self.initialize_states(img_features)\n",
        "      max_cap_len = max([len(caption) for caption in captions]) - 1\n",
        "      predictions = torch.zeros(batch_size, max_cap_len, self.vocab_size).cuda()\n",
        "      alphas = torch.zeros(batch_size, max_cap_len, img_features.size(1)).cuda()\n",
        "        \n",
        "      word_embeddings = self.word_embed(captions)\n",
        "      if dropout:\n",
        "        word_embeddings = nn.Dropout(0.5)(word_embeddings)\n",
        "      \n",
        "      for t in range(max_cap_len):\n",
        "        context, alpha = self.build_attention_model(img_features, h0)\n",
        "        lstm_input = torch.cat((word_embeddings[:, t,:].squeeze(1), context), dim=1)\n",
        "        \n",
        "        h0, c0 = self.lstm_cell(lstm_input, (h0, c0))\n",
        "        output = self.deep_output(nn.Dropout()(h0))\n",
        "\n",
        "        predictions[:, t, :] = output\n",
        "        alphas[:, t, :] = alpha\n",
        "\n",
        "        return predictions, alphas\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}